[["intro.html", "Chapter 1 Build a Model 1.1 Data understanding 1.2 Model type 1.3 Model training 1.4 Model predictions 1.5 Model evaluation", " Chapter 1 Build a Model The following content is adapted from the excellent book “Hands-on machine learning with scikit-learn, keras and tensorflow” from Géron (2019). In this chapter you will learn how to specify a regression model with the tidymodels package. To use the code in this article, you will need to install the following packages: tidyverse tidymodels skimr GGally library(tidyverse) library(tidymodels) library(skimr) library(GGally) In this example, our goal is to build a model of housing prices in California. In particular, the model should learn from California census data and be able to predict the median house price in any district (population of 600 to 3000 people), given some predictor variables. We use the root mean square error (RMSE) as a performance measure for our regression problem. 1.1 Data understanding In Data Understanding, you will learn how to: Import data Get an overview about the data structure Split data into training and test set 1.1.1 Imort Data First of all, let’s import the data: LINK &lt;- &quot;https://raw.githubusercontent.com/kirenz/datasets/master/housing.csv&quot; housing &lt;- read_csv(LINK) 1.1.2 Data overview Next, we take a look at the data structure: California census top 4 rows of the DataFrame: head(housing, 4) Data info: glimpse(housing) Data summary of numerical and categorical attributes using a function from the package skimr: skim(housing) Count levels of our categorical variable: housing %&gt;% count(ocean_proximity, sort = TRUE ) The function ggscatmat from the package GGally creates a matrix with scatterplots, densities and correlations for numeric columns. In our code, we enter the dataset housing, an color column for our categorical variable ocean_proximity, and an alpha level of 0.8 (for transparency). ggscatmat(housing, color = &quot;ocean_proximity&quot;, alpha = 0.8) To obtain an overview of even more visualizations, we can use the function ggpairs: ggpairs(housing) 1.1.3 Data splitting Let’s assume we would know that the median income is a very important attribute to predict median housing prices. Take a look at ?? Histogram for median income housing %&gt;% ggplot(aes(median_income)) + geom_histogram(bins = 30) Therefore, we would want to ensure that the test set is representative of the the various categories of incomes in the whole dataset. Hence, we use stratified sampling Let’s create a training and test set using stratified sampling. set.seed(42) new_split &lt;- initial_split(housing, prop = 3 / 4, strata = median_income, breaks = 5 ) new_train &lt;- training(new_split) new_test &lt;- testing(new_split) 1.2 Model type Pick an model type: choose from this list Set the engine: choose from this list Set the mode: regression or classification library(tidymodels) lm_spec &lt;- # your model specification linear_reg() %&gt;% # model type set_engine(engine = &quot;lm&quot;) %&gt;% # model engine set_mode(&quot;regression&quot;) # model mode # Show your model specification lm_spec 1.3 Model training In the training process, you run an algorithm on data and thereby produce a model. This process is also called model fitting. lm_fit &lt;- # your fitted model fit( lm_spec, # your model specification Sale_Price ~ Gr_Liv_Area, # a Linear Regression formula data = ames # your data ) # Show your fitted model lm_fit 1.4 Model predictions We use our fitted model to make predictions. price_pred &lt;- lm_fit %&gt;% predict(new_data = ames) %&gt;% mutate(price_truth = ames$Sale_Price) head(price_pred) If we would want to make predictions for new houses, we could proceed as follows: # New values (our x variable) new_homes &lt;- tibble(Gr_Liv_Area = c(334, 1126, 1442, 1500, 1743, 5642)) # Prediction for new houses (predict y) lm_fit %&gt;% predict(new_data = new_homes) 1.5 Model evaluation We use the Root Mean Squared Error (RMSE) to evaluate our regression model. Therefore, we use the function \\(rmse(data, truth, estimate)\\). rmse( data = price_pred, truth = price_truth, estimate = .pred ) References "],["process-with-data-splitting.html", "Chapter 2 Process with data splitting 2.1 Data splitting 2.2 Model type 2.3 Model training 2.4 Model predictions 2.5 Model evaluation 2.6 Model specification 2.7 Data split 2.8 recipe() 2.9 prep() 2.10 juice() 2.11 bake() 2.12 Specify model 2.13 Fit model 2.14 Evaluate model 2.15 Evaluate final model 2.16 recipe() 2.17 Helper functions 2.18 step_novel() 2.19 step_dummy() 2.20 step_zv() 2.21 step_normalize() 2.22 workflow() 2.23 fit() 2.24 predict() &amp; rmse() 2.25 Data overview 2.26 Create features 2.27 Logistic regression 2.28 Decision tree 2.29 Random forest 2.30 Boosted tree (XGBoost) 2.31 Fit models with workflows 2.32 Train models 2.33 Model recipe objects 2.34 Summary 2.35 Logistic regression", " Chapter 2 Process with data splitting The best way to measure a model’s performance at predicting new data is to actually predict new data. This function “splits” data randomly into a single testing and a single training set: initial_split(data, prop = 3/4, strata, breaks). We also use stratified sampling in this example. 2.1 Data splitting set.seed(100) ames_split &lt;- initial_split(ames, strata = Sale_Price, breaks = 4 ) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) 2.2 Model type lm_spec_2 &lt;- linear_reg() %&gt;% set_engine(engine = &quot;lm&quot;) %&gt;% set_mode(&quot;regression&quot;) 2.3 Model training lm_fit_2 &lt;- lm_spec_2 %&gt;% fit(Sale_Price ~ Gr_Liv_Area, data = ames_train ) # only use training data 2.4 Model predictions price_pred_2 &lt;- lm_fit %&gt;% predict(new_data = ames_test) %&gt;% mutate(price_truth = ames_test$Sale_Price) 2.5 Model evaluation rmse(price_pred_2, truth = price_truth, estimate = .pred ) 2.6 Model specification 2.7 Data split 2.8 recipe() 2.9 prep() 2.10 juice() 2.11 bake() 2.12 Specify model 2.13 Fit model 2.14 Evaluate model 2.15 Evaluate final model 2.16 recipe() 2.17 Helper functions 2.18 step_novel() 2.19 step_dummy() 2.20 step_zv() 2.21 step_normalize() 2.22 workflow() 2.23 fit() 2.24 predict() &amp; rmse() 2.25 Data overview 2.26 Create features 2.26.1 Date 2.26.2 Dummy variables 2.26.3 Zero variance 2.26.4 Correlations 2.27 Logistic regression 2.28 Decision tree 2.29 Random forest 2.30 Boosted tree (XGBoost) 2.31 Fit models with workflows 2.31.1 Logistic regression 2.31.2 Decision tree 2.31.3 Random forest 2.31.4 XGBoost 2.32 Train models 2.32.1 Logistic regression 2.32.2 Decision tree 2.32.3 Random forest 2.32.4 XG Boost 2.33 Model recipe objects 2.33.1 Logistic regression 2.33.2 Decision tree 2.33.3 Random forest 2.33.4 XG Boost 2.34 Summary 2.35 Logistic regression 2.35.1 ROC curve 2.35.2 AUC 2.35.3 Accuracy 2.35.4 Recall 2.35.5 Precision "]]
