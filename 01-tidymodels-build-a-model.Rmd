
# (PART) BUILD A MODEL {-} 

# Build a Model {#intro}


```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, 
                      echo = TRUE, dpi = 300, cache.lazy = FALSE,
                      tidy = "styler", fig.width = 8, fig.height = 5,
                      eval=FALSE)
library(tidyverse)
library(tidymodels)
```

In this chapter you will learn how to specify a regression model with the tidymodels package. To use the code in this article, you will need to install the following packages: 

* [`tidyverse`](https://www.tidyverse.org/) 
* [`tidymodels`](https://www.tidymodels.org/) 
* [`skimr`](https://cran.r-project.org/web/packages/skimr/vignettes/skimr.html) 
* [`GGally`](https://ggobi.github.io/ggally/index.html)

```{r}
library(tidyverse)
library(tidymodels)
library(skimr)
library(GGally)
```


*The following content is adapted from the excellent book “Hands-on machine learning with scikit-learn, keras and tensorflow” from @Geron2019.*

In this example, our goal is to build a model of housing prices in California. In particular, the model should learn from California census data and be able to predict the median house price in any district (population of 600 to 3000 people), given some predictor variables. We use the root mean square error (RMSE) as a performance measure for our regression problem.

## Data understanding


:::puzzle
- Import data 
- Get an overview about the data structure
:::



First of all, let's import the data:

```{r}

LINK <- "https://raw.githubusercontent.com/kirenz/datasets/master/housing.csv"
housing <- read_csv(LINK)

```

Next, we take a look at the data structure:

California census top 4 rows of the DataFrame: 

```{r}

head(housing, 4)

```

Data info:

```{r}

glimpse(housing)

```

Data summary of numerical and categorical attributes using a function from the package `skimr`:

```{r}

skim(housing)

```

The function `ggscatmat` from the package `GGally` creates a matrix with scatterplots, densities and correlations for numeric columns. In our code, we enter the dataset `housing`, an color column for our categorical variable `ocean_proximity`, and an alpha level of 0.8 (for transparency).

```{r eval=FALSE}

ggscatmat(housing, color="ocean_proximity", alpha=0.8)

```

To obtain an overview of even more visualizations, we can use the function `ggpairs`:

```{r eval=FALSE}

ggpairs(housing)

```



## Model type

1. Pick an `model type`: choose from this [list](https://www.tidymodels.org/find/parsnip/)
2. Set the `engine`: choose from this [list](https://www.tidymodels.org/find/parsnip/)
3. Set the `mode`: regression or classification

```{r}
library(tidymodels)

lm_spec <- # your model specification
  linear_reg() %>%  # model type
  set_engine(engine = "lm") %>%  # model engine
  set_mode("regression") # model mode

# Show your model specification
lm_spec

```


## Model training

In the training process, you run an algorithm on data and thereby produce a model. This process is also called model fitting.


```{r}

lm_fit <- # your fitted model
  fit( 
  lm_spec, # your model specification 
  Sale_Price ~ Gr_Liv_Area, # a Linear Regression formula 
  data = ames # your data
  )

# Show your fitted model
lm_fit

```

## Model predictions

We use our fitted model to make predictions. 

```{r}

price_pred <- 
  lm_fit %>% 
  predict(new_data = ames) %>%
  mutate(price_truth = ames$Sale_Price)

head(price_pred)

```

If we would want to make predictions for new houses, we could proceed as follows: 

```{r}

# New values (our x variable)
new_homes <- 
  tibble(Gr_Liv_Area = c(334, 1126, 1442, 1500, 1743, 5642)) 

# Prediction for new houses (predict y)
lm_fit %>%
 predict(new_data = new_homes)

```


## Model evaluation 

We use the Root Mean Squared Error (RMSE) to evaluate our regression model. Therefore, we use the function $rmse(data, truth, estimate)$.

```{r}

rmse(data = price_pred, 
     truth = price_truth, 
     estimate = .pred)

```


# Process with data splitting

The best way to measure a model's performance at predicting new data is to actually predict new data.

This function "splits" data randomly into a single testing and a single training set: `initial_split(data, prop = 3/4, strata, breaks)`. We also use [stratified sampling](https://en.wikipedia.org/wiki/Stratified_sampling) in this example.

## Data splitting

```{r}

set.seed(100) 

ames_split <-  initial_split(ames,
                             strata = Sale_Price,
                             breaks = 4)

ames_train <-  training(ames_split)
ames_test <- testing(ames_split)

```

## Model type

```{r}

lm_spec_2 <-
  linear_reg() %>%  
  set_engine(engine = "lm") %>%  
  set_mode("regression") 

```

## Model training

```{r}
             
lm_fit_2 <- 
  lm_spec_2 %>% 
  fit(Sale_Price ~ Gr_Liv_Area,
      data = ames_train) # only use training data

```

## Model predictions

```{r}
 
price_pred_2 <- 
  lm_fit %>% 
  predict(new_data = ames_test) %>% 
  mutate(price_truth = ames_test$Sale_Price)

```

## Model evaluation

```{r}

rmse(price_pred_2, 
     truth = price_truth, 
     estimate = .pred)

```

